{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "sns.set_theme(font_scale=1.5, rc={'figure.figsize': [12, 8]})\n",
    "sklearn.set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load London Bike Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "bikes_path = data_path / \"london_bikes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"timestamp\" - timestamp field for grouping the data \n",
    "- \"cnt\" - the count of a new bike shares \n",
    "- \"t1\" - real temperature in C \n",
    "- \"t2\" - temperature in C \"feels like\" \n",
    "- \"hum\" - humidity in percentage \n",
    "- \"windspeed\" - wind speed in km/h \n",
    "- \"weathercode\" - category of the weather \n",
    "- \"isholiday\" - boolean field - 1 holiday / 0 non holiday \n",
    "- \"isweekend\" - boolean field - 1 if the day is weekend \n",
    "- \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv(bikes_path, parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['timestamp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['hr'] = bikes['timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['weather_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bikes[['t1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season', 'hr']]\n",
    "y = bikes['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numerical_featurse = ['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend', 'hr']\n",
    "cat_features = ['weather_code', 'season']\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('numerical', 'passthrough', numerical_featurse),\n",
    "    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pois_reg = Pipeline([\n",
    "    ('prep', ct),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', PoissonRegressor())\n",
    "])\n",
    "\n",
    "pois_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This seems low what can we change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numerical_featurse = ['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']\n",
    "cat_features = ['weather_code', 'season', 'hr']\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('numerical', 'passthrough', numerical_featurse),\n",
    "    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n",
    "    \n",
    "])\n",
    "\n",
    "pois_reg = Pipeline([\n",
    "    ('prep', ct),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', PoissonRegressor())\n",
    "])\n",
    "\n",
    "pois_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('numerical', 'passthrough', numerical_featurse),\n",
    "    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n",
    "    \n",
    "])\n",
    "\n",
    "ridge = Pipeline([\n",
    "    ('prep', ct),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "\n",
    "def compute_metrics(y_true, y_pred, sample_weight=None):\n",
    "    \n",
    "    mask = y_pred > 0\n",
    "    if (~mask).any():\n",
    "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
    "        print(f\"WARNING: Estimator yields invalid, non-positive predictions \"\n",
    "              f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
    "              f\"are ignored when computing the Poisson deviance.\")\n",
    "        \n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = sample_weight[mask]\n",
    "   \n",
    "    return {\n",
    "        'mse': mean_squared_error(y_true, y_pred, sample_weight=sample_weight),\n",
    "        'mean poisson deviance': mean_poisson_deviance(y_true, y_pred, sample_weight=sample_weight)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = ridge.predict(X_test)\n",
    "compute_metrics(y_test, ridge_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_pred = pois_reg.predict(X_test)\n",
    "compute_metrics(y_test, poisson_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the prediction distrubutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "ax1.hist(y_test, bins=30, alpha=0.5)\n",
    "ax1.set_title(\"Test data\")\n",
    "ax2.hist(poisson_pred, bins=30, alpha=0.5)\n",
    "ax2.set_title(\"Poisson predictions\")\n",
    "ax3.hist(ridge_pred, bins=30, alpha=0.5)\n",
    "ax3.set_title(\"Ridge predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Train a `sklearn.ensemble.HistGradientBoostingRegressor` on the training set. **Hint:** Set `random_state=42`:\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "```\n",
    "\n",
    "1. Compute the predictinos from this estimator and save it as `hist_pred`\n",
    "1. Compute the metrics for the predicitons on the model using `compute_metrics`.\n",
    "1. Train a `sklearn.ensemble.HistGradientBoostingRegressor` with `loss='poisson'` and `random_state=42` on the trianing set.\n",
    "1. Compute the predictinos from this estimator and save it as `hist_poisson_pred`.\n",
    "1. Compute the metrics for the predicitons on the model using `compute_metrics`.\n",
    "1. **Extra:** Plot the prediction distrubutions for the two models and the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-ex01-solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "def _calibration_curve_weighted(y_true, y_pred, n_bins=10, sample_weight=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    idx_sort = np.argsort(y_pred)\n",
    "    y_pred_bin = np.zeros(n_bins)\n",
    "    y_true_bin = np.zeros(n_bins)\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        sample_weight = np.asarray(sample_weight)\n",
    "    \n",
    "    for i, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n",
    "        if sample_weight is None:\n",
    "            y_pred_bin[i] = np.average(y_pred[idx_sort][sl])\n",
    "            y_true_bin[i] = np.average(y_true[idx_sort][sl])\n",
    "        else:\n",
    "            weights = sample_weight[idx_sort][sl]\n",
    "            y_pred_bin[i] = np.average(y_pred[idx_sort][sl], weights=weights)\n",
    "            y_true_bin[i] = np.average(y_true[idx_sort][sl], weights=weights)\n",
    "    return y_pred_bin, y_true_bin\n",
    "\n",
    "def plot_calibration_curve_weights(y_true, y_pred, n_bins=10, ax=None, title=\"\", sample_weight=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    y_pred_bin, y_true_bin = _calibration_curve_weighted(y_test, y_pred, sample_weight=sample_weight)\n",
    "    \n",
    "    bin_centers = np.arange(1, len(y_pred_bin) + 1)\n",
    "    ax.plot(bin_centers, y_pred_bin, marker='x', linestyle=\"--\", label=\"predictions\")\n",
    "    ax.plot(bin_centers, y_true_bin, marker='o', linestyle=\"--\", label=\"observations\")\n",
    "    ax.set(xlabel=\"Bin number\", xticks=bin_centers, title=title)\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\n",
    "plot_calibration_curve_weights(y_test, ridge_pred, ax=ax1, title=\"Ridge\")\n",
    "plot_calibration_curve_weights(y_test, poisson_pred, ax=ax2, title=\"Poisson Regression\")\n",
    "plot_calibration_curve_weights(y_test, hist_poisson_pred, ax=ax3, title=\"Hist Poisson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_path = data_path / \"claims.csv\"\n",
    "claims = pd.read_csv(claims_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ClaimNb: number of claims on the given policy;\n",
    "- Exposure: total exposure in yearly units;\n",
    "- Area: area code (categorical, ordinal);\n",
    "- VehPower: power of the car (categorical, ordinal);\n",
    "- VehAge: age of the car in years;\n",
    "- DrivAge: age of the (most common) driver in years;\n",
    "- BonusMalus: bonus-malus level between 50 and 230 (with reference level 100);\n",
    "- VehBrand: car brand (categorical, nominal);\n",
    "- VehGas: diesel or regular fuel car (binary);\n",
    "- Density: density of inhabitants per km2 in the city of the living place of the driver;\n",
    "- Region: regions in France (prior to 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = claims['Exposure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = claims[\"ClaimNb\"] / exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = claims.drop([\"Exposure\", \"ClaimNb\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, exposure_train, exposure_test = train_test_split(\n",
    "    X, y, exposure, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train simple dummy regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()\n",
    "dummy.fit(X_train, y_train, sample_weight=exposure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred = dummy.predict(X_test)\n",
    "compute_metrics(y_test, dummy_pred, sample_weight=exposure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Density'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "linear_model_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passthrough_numeric\", \"passthrough\",\n",
    "            [\"BonusMalus\"]),\n",
    "        (\"binned_numeric\", KBinsDiscretizer(n_bins=10),\n",
    "            [\"VehAge\", \"DrivAge\"]),\n",
    "        (\"log_scaled_numeric\", FunctionTransformer(np.log, validate=False),\n",
    "            [\"Density\"]),\n",
    "        (\"onehot_categorical\", OneHotEncoder(handle_unknown='ignore'),\n",
    "            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"preprocessor\", linear_model_preprocessor),\n",
    "    (\"scaler\", MaxAbsScaler()),\n",
    "    (\"reg\", Ridge(alpha=1e-6))])\n",
    "ridge.fit(X_train, y_train, reg__sample_weight=exposure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = ridge.predict(X_test)\n",
    "compute_metrics(y_test, ridge_pred, sample_weight=exposure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_calibration_curve_weights(y_test, ridge_pred, ax=ax, title=\"Ridge\", sample_weight=exposure_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Construct a pipeline with `PoissonRegressor(alpha=1e-12)` with the same preprocess we have above.\n",
    "2. Training the pipeline on the training set. **Hint** remember to set the the sample weight!\n",
    "3. Plot the calibration curve using `plot_calibration_curve_weights`. **Hint** remember to include the sample weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-ex02-solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"categorical\", OrdinalEncoder(),\n",
    "            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n",
    "        (\"numeric\", \"passthrough\",\n",
    "            [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_poisson = Pipeline([\n",
    "    (\"preprocessor\", tree_preprocessor),\n",
    "    (\"reg\", HistGradientBoostingRegressor(loss=\"poisson\", random_state=42))\n",
    "])\n",
    "hist_poisson.fit(X_train, y_train, reg__sample_weight=exposure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_poisson_pred = hist_poisson.predict(X_test)\n",
    "compute_metrics(y_test, hist_poisson_pred, sample_weight=exposure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_calibration_curve_weights(y_test, hist_poisson_pred, ax=ax, title=\"Hist Poisson\", sample_weight=exposure_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_poisson_deviance\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_mean_poisson_deviance = make_scorer(mean_poisson_deviance, sample_weight=exposure_test, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "hist_results = permutation_importance(hist_poisson, X_test, y_test, scoring=neg_mean_poisson_deviance,\n",
    "                                      n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_importance(perm_results, names, ax=None):\n",
    "    perm_sorted_idx = perm_results.importances_mean.argsort()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    _ = ax.boxplot(perm_results.importances[perm_sorted_idx].T, vert=False,\n",
    "                   labels=np.array(names)[perm_sorted_idx])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"] + [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(hist_results, feature_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the feature importances for the trained poisson regression model, `poisson_reg` from exercise 2.\n",
    "1. Plot the feature importances for the model.\n",
    "1. What are the most important featurs according to the poisson regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-ex03-solutions.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop-advanced",
   "language": "python",
   "name": "conda-env-ml-workshop-advanced-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
