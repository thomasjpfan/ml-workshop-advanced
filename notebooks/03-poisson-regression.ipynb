{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Poisson regression\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/thomasjpfan/ml-workshop-advanced/blob/master/notebooks/03-poisson-regression.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for google colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -r https://raw.githubusercontent.com/thomasjpfan/ml-workshop-advanced/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "assert sklearn.__version__.startswith(\"1.0\"), \"Plese install scikit-learn 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(font_scale=1.5, rc={'figure.figsize': [12, 8]})\n",
    "sklearn.set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load London Bike Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "bikes_path = data_path / \"london_bikes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"timestamp\" - timestamp field for grouping the data \n",
    "- \"cnt\" - the count of a new bike shares \n",
    "- \"t1\" - real temperature in C \n",
    "- \"t2\" - temperature in C \"feels like\" \n",
    "- \"hum\" - humidity in percentage \n",
    "- \"windspeed\" - wind speed in km/h \n",
    "- \"weathercode\" - category of the weather \n",
    "- \"isholiday\" - boolean field - 1 holiday / 0 non holiday \n",
    "- \"isweekend\" - boolean field - 1 if the day is weekend \n",
    "- \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    BIKES_URL = \"https://raw.githubusercontent.com/thomasjpfan/ml-workshop-advanced/master/notebooks/data/london_bikes.csv\"\n",
    "    bikes = pd.read_csv(BIKES_URL, parse_dates=['timestamp'])\n",
    "else:\n",
    "    bikes = pd.read_csv(bikes_path, parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['timestamp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['hr'] = bikes['timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['weather_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bikes[['t1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season', 'hr']]\n",
    "y = bikes['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numerical_features = ['t1', 't2', 'hum', 'wind_speed']\n",
    "cat_features = ['weather_code', 'season', 'hr', 'is_holiday', 'is_weekend']\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('numerical', 'passthrough', numerical_features),\n",
    "    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore', drop='if_binary'), cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pois_reg = Pipeline([\n",
    "    ('prep', ct),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', PoissonRegressor())\n",
    "])\n",
    "\n",
    "pois_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('numerical', 'passthrough', numerical_features),\n",
    "    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n",
    "    \n",
    "])\n",
    "\n",
    "ridge = Pipeline([\n",
    "    ('prep', ct),\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('reg', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "\n",
    "def compute_metrics(y_true, y_pred, sample_weight=None):\n",
    "    \n",
    "    mask = y_pred > 0\n",
    "    if (~mask).any():\n",
    "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
    "        print(f\"WARNING: Estimator yields invalid, non-positive predictions \"\n",
    "              f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
    "              f\"are ignored when computing the Poisson deviance.\")\n",
    "        \n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = sample_weight[mask]\n",
    "   \n",
    "    return {\n",
    "        'mse': mean_squared_error(y_true, y_pred, sample_weight=sample_weight),\n",
    "        'mean poisson deviance': mean_poisson_deviance(y_true, y_pred, sample_weight=sample_weight)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = ridge.predict(X_test)\n",
    "compute_metrics(y_test, ridge_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_pred = pois_reg.predict(X_test)\n",
    "compute_metrics(y_test, poisson_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the prediction distrubutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "ax1.hist(y_test, bins=30, alpha=0.5)\n",
    "ax1.set_title(\"Test data\")\n",
    "ax2.hist(poisson_pred, bins=30, alpha=0.5)\n",
    "ax2.set_title(\"Poisson predictions\")\n",
    "ax3.hist(ridge_pred, bins=30, alpha=0.5)\n",
    "ax3.set_title(\"Ridge predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Train a `HistGradientBoostingRegressor` with `random_state=42` on the training set.\n",
    "1. Compute the predictions on the test set and save it as `hist_pred`.\n",
    "1. Compute the metrics for the predicitons on the model using `compute_metrics`.\n",
    "1. Train a `HistGradientBoostingRegressor` with `loss='poisson'` and `random_state=42` on the training set.\n",
    "1. Compute the predictions from this estimator and save it as `hist_poisson_pred`.\n",
    "1. Compute the metrics for the predicitons on the model using `compute_metrics`.\n",
    "1. **Extra:** Plot the prediction distrubutions for the two models and the original data.\n",
    "    - **Hint** You may copy the code right above this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are running locally**, you can uncomment the following cell to load the solution into the cell. On **Google Colab**, [see solution here](https://github.com/thomasjpfan//ml-workshop-advanced/blob/master/notebooks/solutions/03-ex01-solutions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-ex01-solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curve for Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "def _calibration_curve_weighted(y_true, y_pred, n_bins=10, sample_weight=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    idx_sort = np.argsort(y_pred)\n",
    "    y_pred_bin = np.zeros(n_bins)\n",
    "    y_true_bin = np.zeros(n_bins)\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        sample_weight = np.asarray(sample_weight)\n",
    "    \n",
    "    for i, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n",
    "        if sample_weight is None:\n",
    "            y_pred_bin[i] = np.average(y_pred[idx_sort][sl])\n",
    "            y_true_bin[i] = np.average(y_true[idx_sort][sl])\n",
    "        else:\n",
    "            weights = sample_weight[idx_sort][sl]\n",
    "            y_pred_bin[i] = np.average(y_pred[idx_sort][sl], weights=weights)\n",
    "            y_true_bin[i] = np.average(y_true[idx_sort][sl], weights=weights)\n",
    "    return y_pred_bin, y_true_bin\n",
    "\n",
    "def plot_calibration_curve_regression(y_true, y_pred, n_bins=10, ax=None, title=\"\", sample_weight=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    y_pred_bin, y_true_bin = _calibration_curve_weighted(y_test, y_pred, sample_weight=sample_weight)\n",
    "    \n",
    "    bin_centers = np.arange(1, len(y_pred_bin) + 1)\n",
    "    ax.plot(bin_centers, y_pred_bin, marker='x', linestyle=\"--\", label=\"predictions\")\n",
    "    ax.plot(bin_centers, y_true_bin, marker='o', linestyle=\"--\", label=\"observations\")\n",
    "    ax.set(xlabel=\"Bin number\", xticks=bin_centers, title=title)\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a `hist_poisson` to compare calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hist_poisson = HistGradientBoostingRegressor(loss='poisson', random_state=42)\n",
    "hist_poisson.fit(X_train, y_train)\n",
    "\n",
    "hist_poisson_pred = hist_poisson.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\n",
    "plot_calibration_curve_regression(y_test, ridge_pred, ax=ax1, title=\"Ridge\")\n",
    "plot_calibration_curve_regression(y_test, poisson_pred, ax=ax2, title=\"Poisson Regression\")\n",
    "plot_calibration_curve_regression(y_test, hist_poisson_pred, ax=ax3, title=\"Hist Poisson\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_path = data_path / \"claims.csv\"\n",
    "if IN_COLAB:\n",
    "    CLAIMS_URL = \"https://raw.githubusercontent.com/thomasjpfan/ml-workshop-advanced/master/notebooks/data/claims.csv\"\n",
    "    claims = pd.read_csv(CLAIMS_URL)\n",
    "else:\n",
    "    claims = pd.read_csv(claims_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ClaimNb: number of claims on the given policy;\n",
    "- Exposure: total exposure in yearly units;\n",
    "- Area: area code (categorical, ordinal);\n",
    "- VehPower: power of the car (categorical, ordinal);\n",
    "- VehAge: age of the car in years;\n",
    "- DrivAge: age of the (most common) driver in years;\n",
    "- BonusMalus: bonus-malus level between 50 and 230 (with reference level 100);\n",
    "- VehBrand: car brand (categorical, nominal);\n",
    "- VehGas: diesel or regular fuel car (binary);\n",
    "- Density: density of inhabitants per km2 in the city of the living place of the driver;\n",
    "- Region: regions in France (prior to 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = claims['Exposure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = claims[\"ClaimNb\"] / exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = claims.drop([\"Exposure\", \"ClaimNb\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, exposure_train, exposure_test = train_test_split(\n",
    "    X, y, exposure, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train simple dummy regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()\n",
    "dummy.fit(X_train, y_train, sample_weight=exposure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred = dummy.predict(X_test)\n",
    "compute_metrics(y_test, dummy_pred, sample_weight=exposure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Density'].hist(bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "linear_model_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passthrough_numeric\", \"passthrough\",\n",
    "            [\"BonusMalus\"]),\n",
    "        (\"binned_numeric\", KBinsDiscretizer(n_bins=10),\n",
    "            [\"VehAge\", \"DrivAge\"]),\n",
    "        (\"log_scaled_numeric\", FunctionTransformer(np.log),\n",
    "            [\"Density\"]),\n",
    "        (\"onehot_categorical\", OneHotEncoder(handle_unknown='ignore'),\n",
    "            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"preprocessor\", linear_model_preprocessor),\n",
    "    (\"scaler\", MaxAbsScaler()),\n",
    "    (\"reg\", Ridge(alpha=1e-6))])\n",
    "ridge.fit(X_train, y_train, reg__sample_weight=exposure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = ridge.predict(X_test)\n",
    "compute_metrics(y_test, ridge_pred, sample_weight=exposure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve_regression(y_test, ridge_pred, title=\"Ridge\", sample_weight=exposure_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Construct a pipeline with `PoissonRegressor(alpha=1e-4)` with the same preprocesser we have above.\n",
    "    - **Hint**: You may reuse `linear_model_preprocessor` and `MaxAbsScaler`\n",
    "2. Training the pipeline on the training set. **Hint**: Remember to set the the sample weight!\n",
    "3. Plot the calibration curve using `plot_calibration_curve_regression`. **Hint**: Remember to include the sample weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are running locally**, you can uncomment the following cell to load the solution into the cell. On **Google Colab**, [see solution here](https://github.com/thomasjpfan//ml-workshop-advanced/blob/master/notebooks/solutions/03-ex02-solutions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-ex02-solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Build a preprocessor for a tree based model.\n",
    "    - **Hint**: Use `ColumnTransformer`, encode categories with `OrdinalEncoder` and passthrough the numerical features.\n",
    "    - **Extra**: You can use `make_column_selector` to select the correct dtypes.\n",
    "2. Use the preprocessor from step 1 to build a pipeline with `HistGradientBoostingRegressor` with `loss=\"poisson\"`.\n",
    "3. Fit the model from step 2 while also setting `sample_weight` to `exposure_train`.\n",
    "4. Use `compute_metrics` to compute the mse and the mean poisson deviance.\n",
    "    - **Hint** Rememver to incldue the sample weight!\n",
    "5. Plot the calibration curve using `plot_calibration_curve_regression`.\n",
    "    - **Hint** remember to include the sample weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are running locally**, you can uncomment the following cell to load the solution into the cell. On **Google Colab**, [see solution here](https://github.com/thomasjpfan//ml-workshop-advanced/blob/master/notebooks/solutions/03-ex03-solutions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-ex03-solutions.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
